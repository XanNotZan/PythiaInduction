{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-kdtDny1S26n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "outputId": "9dae0c55-0de4-4512-b4db-bfb1bcf2150b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-2.16.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting circuitsvis\n",
            "  Downloading circuitsvis-1.43.3-py3-none-any.whl.metadata (983 bytes)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (1.12.0)\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.8.1)\n",
            "Collecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.3.5-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting numpy<2,>=1.26 (from transformer_lens)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.2.2)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.2.1)\n",
            "Requirement already satisfied: torch>=2.6 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.51 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.57.3)\n",
            "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer_lens)\n",
            "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.15.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.23.1)\n",
            "Requirement already satisfied: importlib-metadata>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from circuitsvis) (8.7.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2025.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=5.1.0->circuitsvis) (3.23.0)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (2.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (0.22.1)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.47.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.23.0->transformer_lens) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.6->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.6->transformer_lens) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
            "Downloading transformer_lens-2.16.1-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.0/192.0 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading circuitsvis-1.43.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Downloading jaxtyping-0.3.5-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: transformers-stream-generator\n",
            "  Building wheel for transformers-stream-generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12426 sha256=4e87a641df827bc40d0174ff8874aadd29e7902afdbd0f4b0ad83e0c89aad15c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/58/d2/014cb67c3cc6def738c1b1635dbf4e3dab6fb63aba7070dce0\n",
            "Successfully built transformers-stream-generator\n",
            "Installing collected packages: better-abc, wadler-lindig, numpy, fancy-einsum, beartype, jaxtyping, circuitsvis, transformers-stream-generator, transformer_lens\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: beartype\n",
            "    Found existing installation: beartype 0.22.9\n",
            "    Uninstalling beartype-0.22.9:\n",
            "      Successfully uninstalled beartype-0.22.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "plum-dispatch 2.6.0 requires beartype>=0.16.2, but you have beartype 0.14.1 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beartype-0.14.1 better-abc-0.0.3 circuitsvis-1.43.3 fancy-einsum-0.0.3 jaxtyping-0.3.5 numpy-1.26.4 transformer_lens-2.16.1 transformers-stream-generator-0.0.5 wadler-lindig-0.1.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "4c742e31c1964e91abd84b37cf17d09c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install transformer_lens circuitsvis plotly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformer_lens as tl\n",
        "from transformer_lens import HookedTransformer\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.io as pio"
      ],
      "metadata": {
        "id": "h7-m2cQPUtdN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = 'First Alice talked to Bob, then Carol talked to Bob, then Alice talked to'\n",
        "prompt2 = 'First Carol talked to Bob, then Alice talked to Bob, then Carol talked to'"
      ],
      "metadata": {
        "id": "SgesOVrOJK2G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4d3c94f",
        "outputId": "709d9b87-e038-4717-939e-09003e73c314"
      },
      "source": [
        "words = [' Carol', ' Alice', ' both']\n",
        "\n",
        "for word in words:\n",
        "    tokens = model.to_str_tokens(word, prepend_bos=False)\n",
        "    print(f\"'{word}' parses to: {tokens}\")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "' Carol' parses to: [' Carol']\n",
            "' Alice' parses to: [' Alice']\n",
            "' both' parses to: [' both']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(cache, layer, n_heads):\n",
        "    attn = cache[\"pattern\", layer][0].cpu().detach().numpy()\n",
        "    n_heads = min(n_heads, attn.shape[0])\n",
        "\n",
        "    cols = 4\n",
        "    rows = (n_heads + cols - 1) // cols\n",
        "\n",
        "    fig = make_subplots(rows=rows, cols=cols, subplot_titles=[f\"Head {i+1}\" for i in range(n_heads)])\n",
        "\n",
        "    for i in range(n_heads):\n",
        "        fig.add_trace(\n",
        "            go.Heatmap(z=attn[i], showscale=False),\n",
        "            row=i//cols + 1, col=i%cols + 1\n",
        "        )\n",
        "\n",
        "    fig.update_layout(height=200*rows, width=800, title=f\"Layer {layer} Attention Heads\")\n",
        "    fig.show(renderer=\"colab\")\n",
        "\n",
        "def get_token_stats(model, term, logits, cache):\n",
        "    # token id\n",
        "    token_id = model.to_single_token(term)\n",
        "    if token_id is None:\n",
        "        print(f\"'{term}' is not a single token.\")\n",
        "        return\n",
        "\n",
        "    # next token logits\n",
        "    next_token_logits = logits[0, -1]\n",
        "    # score of term\n",
        "    score = next_token_logits[token_id].item()\n",
        "\n",
        "    # determine rank of term\n",
        "    rank = (next_token_logits > score).sum().item() + 1\n",
        "\n",
        "    print(f\"Stats for '{term}' -> Rank: {rank}, Logit Score: {score:.4f}\")\n",
        "\n",
        "def pipeline(model, prompt, layer, n_heads, graph=False):\n",
        "  tokens = model.to_tokens(prompt)\n",
        "  str_tokens = model.to_str_tokens(prompt)\n",
        "  logits, cache = model.run_with_cache(tokens)\n",
        "\n",
        "  # top 5 predictions\n",
        "  top_logits, top_indices = logits[0, -1].topk(5)\n",
        "  print(\"Top 5 most likely next tokens:\")\n",
        "  for i in range(5):\n",
        "      token = model.to_string(top_indices[i])\n",
        "      score = top_logits[i].item()\n",
        "      print(f\"Rank {i+1}: '{token}' | Logit: {score:.4f}\")\n",
        "\n",
        "  print()\n",
        "  get_token_stats(model, ' Carol', logits, cache)\n",
        "  get_token_stats(model, ' Alice', logits, cache)\n",
        "  get_token_stats(model, ' both', logits, cache)\n",
        "  if graph:\n",
        "    plot_attention(cache, layer, n_heads)"
      ],
      "metadata": {
        "id": "iOgrMuP1bSie"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer.from_pretrained(\"pythia-70m\")\n",
        "prompt = prompt1\n",
        "pipeline(model, prompt, 5, 8)\n",
        "print()\n",
        "prompt = prompt2\n",
        "pipeline(model, prompt, 5, 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5rVllpdWnFH",
        "outputId": "2e428dd9-4982-47c3-e52d-c10a0084317e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model pythia-70m into HookedTransformer\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Bob' | Logit: 22.3514\n",
            "Rank 2: ' him' | Logit: 21.3399\n",
            "Rank 3: ' her' | Logit: 21.1042\n",
            "Rank 4: ' Carol' | Logit: 20.4703\n",
            "Rank 5: ' the' | Logit: 19.3488\n",
            "\n",
            "Stats for ' Carol' -> Rank: 4, Logit Score: 20.4703\n",
            "Stats for ' Alice' -> Rank: 28, Logit Score: 16.8597\n",
            "Stats for ' both' -> Rank: 32, Logit Score: 16.7022\n",
            "\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Bob' | Logit: 22.3075\n",
            "Rank 2: ' him' | Logit: 21.8679\n",
            "Rank 3: ' her' | Logit: 20.6425\n",
            "Rank 4: ' the' | Logit: 19.1840\n",
            "Rank 5: ' his' | Logit: 19.1405\n",
            "\n",
            "Stats for ' Carol' -> Rank: 6, Logit Score: 18.7575\n",
            "Stats for ' Alice' -> Rank: 10, Logit Score: 18.1007\n",
            "Stats for ' both' -> Rank: 26, Logit Score: 16.7347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer.from_pretrained(\"pythia-160m\")\n",
        "prompt = prompt1\n",
        "pipeline(model, prompt, 10, 12)\n",
        "print()\n",
        "prompt = prompt2\n",
        "pipeline(model, prompt, 10, 12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmWORYaTWnc-",
        "outputId": "607813b1-6f34-4a92-d198-4d25e7129938"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model pythia-160m into HookedTransformer\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Bob' | Logit: 31.4864\n",
            "Rank 2: ' Carol' | Logit: 31.2838\n",
            "Rank 3: ' the' | Logit: 29.7144\n",
            "Rank 4: ' John' | Logit: 29.2363\n",
            "Rank 5: ' Peter' | Logit: 29.1680\n",
            "\n",
            "Stats for ' Carol' -> Rank: 2, Logit Score: 31.2838\n",
            "Stats for ' Alice' -> Rank: 7, Logit Score: 28.7729\n",
            "Stats for ' both' -> Rank: 53, Logit Score: 27.0730\n",
            "\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Bob' | Logit: 31.7674\n",
            "Rank 2: ' Alice' | Logit: 31.6405\n",
            "Rank 3: ' Carol' | Logit: 30.5645\n",
            "Rank 4: ' the' | Logit: 29.8524\n",
            "Rank 5: ' her' | Logit: 29.7779\n",
            "\n",
            "Stats for ' Carol' -> Rank: 3, Logit Score: 30.5645\n",
            "Stats for ' Alice' -> Rank: 2, Logit Score: 31.6405\n",
            "Stats for ' both' -> Rank: 122, Logit Score: 26.7902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer.from_pretrained(\"pythia-410m\")\n",
        "prompt = prompt1\n",
        "pipeline(model, prompt, 10, 16)\n",
        "print()\n",
        "prompt = prompt2\n",
        "pipeline(model, prompt, 10, 16)"
      ],
      "metadata": {
        "id": "zkEfEMeJkUpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98752cb9-dc25-4039-ae42-bc4db9eb4ca5"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model pythia-410m into HookedTransformer\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Bob' | Logit: 18.5772\n",
            "Rank 2: ' Carol' | Logit: 18.3591\n",
            "Rank 3: ' Alice' | Logit: 16.2441\n",
            "Rank 4: ' her' | Logit: 15.1980\n",
            "Rank 5: ' him' | Logit: 14.3990\n",
            "\n",
            "Stats for ' Carol' -> Rank: 2, Logit Score: 18.3591\n",
            "Stats for ' Alice' -> Rank: 3, Logit Score: 16.2441\n",
            "Stats for ' both' -> Rank: 43, Logit Score: 11.6170\n",
            "\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Bob' | Logit: 18.2777\n",
            "Rank 2: ' Carol' | Logit: 16.7947\n",
            "Rank 3: ' Alice' | Logit: 16.5323\n",
            "Rank 4: ' her' | Logit: 14.9715\n",
            "Rank 5: ' him' | Logit: 14.0250\n",
            "\n",
            "Stats for ' Carol' -> Rank: 2, Logit Score: 16.7947\n",
            "Stats for ' Alice' -> Rank: 3, Logit Score: 16.5323\n",
            "Stats for ' both' -> Rank: 43, Logit Score: 11.6205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer.from_pretrained(\"pythia-1b\")\n",
        "prompt = prompt1\n",
        "pipeline(model, prompt, 10, 8)\n",
        "print()\n",
        "prompt = prompt2\n",
        "pipeline(model, prompt, 10, 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S--D2gPeVoBt",
        "outputId": "84fb7094-f4a0-496f-8d3e-7d7adc5c49a1"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model pythia-1b into HookedTransformer\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Carol' | Logit: 18.3959\n",
            "Rank 2: ' Bob' | Logit: 18.2088\n",
            "Rank 3: ' Alice' | Logit: 14.5569\n",
            "Rank 4: ' the' | Logit: 13.8756\n",
            "Rank 5: ' Charlie' | Logit: 13.3573\n",
            "\n",
            "Stats for ' Carol' -> Rank: 1, Logit Score: 18.3959\n",
            "Stats for ' Alice' -> Rank: 3, Logit Score: 14.5569\n",
            "Stats for ' both' -> Rank: 6, Logit Score: 13.0936\n",
            "\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Bob' | Logit: 19.3744\n",
            "Rank 2: ' Alice' | Logit: 18.0063\n",
            "Rank 3: ' Carol' | Logit: 14.6690\n",
            "Rank 4: ' the' | Logit: 13.9192\n",
            "Rank 5: ' her' | Logit: 13.0405\n",
            "\n",
            "Stats for ' Carol' -> Rank: 3, Logit Score: 14.6690\n",
            "Stats for ' Alice' -> Rank: 2, Logit Score: 18.0063\n",
            "Stats for ' both' -> Rank: 8, Logit Score: 12.8641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer.from_pretrained(\"pythia-1.4b\")\n",
        "prompt = prompt1\n",
        "pipeline(model, prompt, 10, 16)\n",
        "print()\n",
        "prompt = prompt2\n",
        "pipeline(model, prompt, 10, 16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCPQE9FyVoiz",
        "outputId": "1621b71c-3dd8-4106-860a-fb444ce0a9e8"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model pythia-1.4b into HookedTransformer\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Carol' | Logit: 19.2994\n",
            "Rank 2: ' Bob' | Logit: 17.6222\n",
            "Rank 3: ' Alice' | Logit: 15.1997\n",
            "Rank 4: ' both' | Logit: 14.3152\n",
            "Rank 5: ' the' | Logit: 13.1826\n",
            "\n",
            "Stats for ' Carol' -> Rank: 1, Logit Score: 19.2994\n",
            "Stats for ' Alice' -> Rank: 3, Logit Score: 15.1997\n",
            "Stats for ' both' -> Rank: 4, Logit Score: 14.3152\n",
            "\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Alice' | Logit: 18.4025\n",
            "Rank 2: ' Bob' | Logit: 18.0595\n",
            "Rank 3: ' Carol' | Logit: 14.7172\n",
            "Rank 4: ' both' | Logit: 13.7493\n",
            "Rank 5: '\n",
            "' | Logit: 13.4370\n",
            "\n",
            "Stats for ' Carol' -> Rank: 3, Logit Score: 14.7172\n",
            "Stats for ' Alice' -> Rank: 1, Logit Score: 18.4025\n",
            "Stats for ' both' -> Rank: 4, Logit Score: 13.7493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer.from_pretrained(\"pythia-2.8b\")\n",
        "prompt = prompt1\n",
        "pipeline(model, prompt, 10, 8)\n",
        "print()\n",
        "prompt = prompt2\n",
        "pipeline(model, prompt, 10, 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xq0O4GczIyHQ",
        "outputId": "7d6a3251-411b-461a-9e35-c2e2138f12ae"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model pythia-2.8b into HookedTransformer\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Carol' | Logit: 17.8021\n",
            "Rank 2: ' Bob' | Logit: 15.9951\n",
            "Rank 3: ' Alice' | Logit: 14.7027\n",
            "Rank 4: ' both' | Logit: 13.7414\n",
            "Rank 5: ' herself' | Logit: 13.2054\n",
            "\n",
            "Stats for ' Carol' -> Rank: 1, Logit Score: 17.8021\n",
            "Stats for ' Alice' -> Rank: 3, Logit Score: 14.7027\n",
            "Stats for ' both' -> Rank: 4, Logit Score: 13.7414\n",
            "\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Alice' | Logit: 18.2920\n",
            "Rank 2: ' Bob' | Logit: 15.1436\n",
            "Rank 3: ' Al' | Logit: 13.8783\n",
            "Rank 4: ' both' | Logit: 13.8685\n",
            "Rank 5: '\n",
            "' | Logit: 13.3322\n",
            "\n",
            "Stats for ' Carol' -> Rank: 6, Logit Score: 13.1340\n",
            "Stats for ' Alice' -> Rank: 1, Logit Score: 18.2920\n",
            "Stats for ' both' -> Rank: 4, Logit Score: 13.8685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = HookedTransformer.from_pretrained(\"pythia-6.9b\")\n",
        "# prompt = prompt1\n",
        "# pipeline(model, prompt, 10, 8)\n",
        "# print()\n",
        "# prompt = prompt2\n",
        "# pipeline(model, prompt, 10, 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852,
          "referenced_widgets": [
            "cb043bc4b66e44858681cb076f6c411b",
            "a66056bc3c174f7b8b5227b86a5e830b",
            "ed829c417958418aa7d0d7e129539223",
            "1c1321ce47a84429b8539edeb8e0ffab",
            "caaad02bce95496b8af6341e46a75656",
            "6827f9105ba34c238d0fbc89989a88f7",
            "2cf72efcb2d84b3aa5cb211c7114de44",
            "311a5dc472564148a3bf737182c910d7",
            "b8d72c61acc44dde892c78f479bb4a51",
            "f94846769aae40cc92f457a99ac486b7",
            "4dd6bbb3f0fc4558a23b5a30d4bd5597",
            "6793479de2954fdf9047fe1f6b4388f3",
            "aa3f06fba7d9413fa93448162a43cd8f",
            "0198497e3dd84719800f5a9d8d8248ee",
            "6aee60640d8349f596664506fd429583",
            "11e798cd310c48c3860cfa2bef3b5304",
            "f22d0e29455e4307a45fa06142aac1a7",
            "bbb05d7e878a48c9bce65be45b8fee19",
            "aa60546a1e1d4e518755190ba7629e72",
            "552effcc7a694ecdaa9c67db7272f580",
            "80b51bb79cd44673a432725e0a629417",
            "ccd55ed3970d466bb360ec844f733598",
            "dd0496637abd4301b348094a44ac8314",
            "0a80d2aaee0c49cfa8364fb3e1b18c6e",
            "3b1453b415394e7592715508cce1a1d5",
            "97fceed1c770416aaab65b677de01cd0",
            "ad3d9d6a404f4cbca553fecc1c2a553b",
            "5b9ecd0672d545d2b54c75b5a53f230c",
            "88fecfe781fd429d876ce42f15a06499",
            "e4e6a405e2cf4632bde0bf50e175be29",
            "8cd99a3bf64a48d39ed7d6f61cec26b4",
            "87416ac920214957902019cdff7563c2",
            "a4f6d2684e4c4b0ab51e7f4ddf87c8f5",
            "f93ce9fc1eac4220a156f9652682a3da",
            "86ac532aeb1a42139865f0375993df9c",
            "022151ee043545c081204c1e05887d16",
            "6561dab0c1de4b0481f40bd9cb0947d7",
            "81d98d0017f048e7977f16590a6d2338",
            "fd605756d5f54697922ae7331adf6363",
            "f77654e4c5ee4b428e304fc2e90d12c5",
            "8cae105ab40a4c819ec1a6ad13fc85a3",
            "025c99584a624ffda3c58b512422aa38",
            "5c03c91ba99b465fa77b6bd36b630f3b",
            "2e24ea9280574d1e907dc1a3460b8c3e",
            "fa8e4538c1d049b4bf1fbaf5dd8f6874",
            "415f8ff1d46d492fa67dc2d271839a15",
            "21b7a520ce444ba3855b5d11b3a767d9",
            "a237e3e71632457d970868da46324ab0",
            "734c93cfb7d040c2b9df990bd80c3b9e",
            "373a4c154841487a84e3a520c12bfe2c",
            "5568c24a7c2547d6a47b4a493317834e",
            "145d47913ab64d29a0c082ec487b4a7c",
            "a5f7177d61e641b49f1608e606ca7e2d",
            "552f6d701e9b4dea9bb9b16424b774df",
            "e96f566bb77f482d8639c30ed4f11116",
            "7bcef12ca2484d1da03bd3898e26a5b6",
            "7fab28338f03462f9469381083087f55",
            "8a0f6a9f3f5b40c7a5b43e73c10b17d6",
            "7da6206ee7cc4129944622c9fb0bb644",
            "6996d42d8f97433c893be901ab94abfb",
            "b103bff57b5a408ca91a172cd8d54a8d",
            "e28936cdec45498eac3f07466cee157f",
            "a3757f85d42f46abae05a8b3c308c201",
            "a7046fa2402848ee9102d969b54661e7",
            "e709fa3db015408eaed0c8e880199396",
            "87a0dd36cd2d475f8a3478483b35b08d",
            "8f273122fb1f4fbc94e3a09e497964c9",
            "9258bac2205e4bfab13967cab4131e27",
            "a6a6e5e6aa2b45859059c4ee56fcfd05",
            "c865cf7516ac4dc8aa386ce3ec603f5b",
            "8296cfce39034d19aba272ea8ca226cf",
            "1f19753c05be4066b8dbc4390d78fda7",
            "11ec660b3f2640bda1e5f6565eb29b1c",
            "b9a679819b534d3089e999873e30c6a0",
            "97c419433eca4804a21821f13cb83c53",
            "07de604da4e743b9b17f1e0dd476fc3e",
            "6e18eddbe8af42a2be5513681bf81b39",
            "162043463817440a8cb6876adf6ab5f2",
            "e6761c2f58f4411fa906805a755dee30",
            "9c1f3d2c112a4217a8763869189bb825",
            "36ef79b5c59a458bb6d6ac76d413f1da",
            "30860af3c52f47629cabb43c1a044bbf",
            "ed050fe6b4154d808cd775d1ae21f676",
            "aec42ad065c54005a04ca6bcfbc91a86",
            "544c8dde835c4804a20e29126436fbdd",
            "3fcdab2c933344cca11f050b8fb7c19c",
            "93e2f5ebd08a4f738de1782507afc77a",
            "2bc56ad5f42e4ec98b4572c3bc8a3b96",
            "2c1fefb03452400081027e87be895f20",
            "fb86ee18072b4783a714d761db683334",
            "dedb06cafb4046b3a3f2fc60621b20c1",
            "e804a4a03c2a40b78e70e13bff3f5d3d",
            "8a544223b3db404288c11ca6d5f1fb20",
            "735cc4fff32d42149c3fe89c5753ef42",
            "4c9a14c8f3e14674b79c87d93bc993fa",
            "017b748ab65a4771b7e8d717937fa658",
            "c4b12a796cc741c8a8ae5a19a8d45538",
            "4b6a000a90b04982b26a37c053caf879",
            "c58cdc866dc2440abf1e66d03631419f"
          ]
        },
        "collapsed": true,
        "id": "53jkIztrYgyv",
        "outputId": "f39a97f3-8b2a-4105-e636-19edc814cace"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb043bc4b66e44858681cb076f6c411b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6793479de2954fdf9047fe1f6b4388f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd0496637abd4301b348094a44ac8314"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f93ce9fc1eac4220a156f9652682a3da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.91G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa8e4538c1d049b4bf1fbaf5dd8f6874"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bcef12ca2484d1da03bd3898e26a5b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f273122fb1f4fbc94e3a09e497964c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "162043463817440a8cb6876adf6ab5f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c1fefb03452400081027e87be895f20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model pythia-6.9b into HookedTransformer\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Carol' | Logit: 17.9819\n",
            "Rank 2: ' Bob' | Logit: 17.2026\n",
            "Rank 3: ' Charlie' | Logit: 13.3058\n",
            "Rank 4: ' Alice' | Logit: 13.2837\n",
            "Rank 5: '\n",
            "' | Logit: 13.2825\n",
            "\n",
            "Stats for ' Carol' -> Rank: 1, Logit Score: 17.9819\n",
            "Stats for ' Alice' -> Rank: 4, Logit Score: 13.2837\n",
            "Stats for ' both' -> Rank: 9, Logit Score: 12.7779\n",
            "\n",
            "Top 5 most likely next tokens:\n",
            "Rank 1: ' Alice' | Logit: 18.7275\n",
            "Rank 2: ' Bob' | Logit: 17.8020\n",
            "Rank 3: ' Carol' | Logit: 14.7108\n",
            "Rank 4: '\n",
            "' | Logit: 14.4201\n",
            "Rank 5: ' both' | Logit: 13.7236\n",
            "\n",
            "Stats for ' Carol' -> Rank: 3, Logit Score: 14.7108\n",
            "Stats for ' Alice' -> Rank: 1, Logit Score: 18.7275\n",
            "Stats for ' both' -> Rank: 5, Logit Score: 13.7236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import gc\n",
        "\n",
        "# # 1. Delete variables holding heavy data\n",
        "# # Add any other variable names you want to clear to this list\n",
        "# for var in ['model', 'logits', 'cache']:\n",
        "#     if var in globals():\n",
        "#         del globals()[var]\n",
        "\n",
        "# # 2. Run Python's garbage collector to clean up deleted objects\n",
        "# gc.collect()\n",
        "\n",
        "# # 3. Clear PyTorch's internal GPU cache\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.empty_cache()\n",
        "#     print(\"GPU memory cleared!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDOOA1_iOUMn",
        "outputId": "463d2478-7cce-4d35-f117-eecff07cc988"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory cleared!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = HookedTransformer.from_pretrained(\"pythia-12b\")\n",
        "# prompt = prompt1\n",
        "# pipeline(model, prompt, 10, 8)\n",
        "# print()\n",
        "# prompt = prompt2\n",
        "# pipeline(model, prompt, 10, 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772,
          "referenced_widgets": [
            "91627c55a3df485b9bd6850f9ae76265",
            "26ede91843284f3a97a1c4f9604148d8",
            "41c73539f4f24f8fb879c98b5e127144",
            "cf5694a751c5437b91201ff235e56dc9",
            "74c49fb1ef594942b6d12d600948cf00",
            "c199fe41572c4464a86a617462883303",
            "28c59ad38f4c4261a0e4b184b761bacf",
            "68a1aa39bab04a9bb9551c8a631d518b",
            "cf0fa45a153c48eba72eaada6ca2b9d1",
            "12807827a28246cfbd5ba4e9c67819dc",
            "7a1ac5dd6eeb43e8ac33dbbdf22f2eef",
            "b6dfd231b76c403c99e50b2ba15f0d50",
            "3c726d940983493f934f98e352e36158",
            "6dc4bc52ee34484f9adcdf6e7c83a6d8",
            "7c3f545f79c94dbd8cdbb570c447f970",
            "3a937e5b14f2450f937306eb16c89308",
            "1c202f6510984dc2a65c06c4a099ec5c",
            "ece251839a3e4770934f52ac485021a2",
            "1ed41ebd86c04ebaa39f935928a1109a",
            "eac124f80fef4698b11fa1c3f94b973b",
            "f02f97390b054a898d0685db5b4e4950",
            "1451beafaeb549c0a0c4bb2c971f992a",
            "a7d659b4a41d4ce3a32df1b27c07804c",
            "64b211073fc64eb2b67c251ddce1b7cc",
            "1b973e010c0b455d8e7d6f2bc3e2bca1",
            "1947158d9c964cbaa4ed6a98e7ec2970",
            "57cba481752a447cbd7bb3f8b6467caa",
            "86828e7b9f0b49aa9d28b3466a5fb726",
            "0c5e79010861428f9dcffa34bcc3b244",
            "45e0d51beef5444aa9a6fc0fadf5173a",
            "c01eae5947954597b596767855981431",
            "8d435ac5648548a9bcfcbce67f1db660",
            "4c067fd74df84961be5b1c2806556900",
            "8113d8bbb5834056a78606d332a3049a",
            "197f35b6966249f59d7221513e187c5f",
            "9562bb61a70e41e687f8b68815f17a1d",
            "3744be5aba594848be4dbbff79462b11",
            "b72a9f83852d451f8131aba382667580",
            "25d616c17e3f4d07aff7f19a7db76776",
            "cdc03267cc8d4f849f2027da6f800693",
            "3dd866f60c004320a1a0a6610b56952b",
            "f8a58340df114428b77f2b22bae11c71",
            "b36c68073b66475884c0701989a0da2e",
            "7107e06277874321880b9487429c4295",
            "0a272c287e4f4a1cb01c0e0ed24e38b0",
            "24815a9745a14267800078b4e904fce4",
            "f115aaf752cb42459fd11bdd680cc7ce",
            "bd0bf95f9b9e4187ac5fd1133efd3339",
            "ff0ac8a1905e4a3cacbfdd8d30d701b6",
            "c7bbab6c4ec5418b939aff19ca239c03",
            "bca430b2db024a1a805eb279181f40d8",
            "12fc81e9173240eaa617235cff999463",
            "f2cdb3f288fc421eb8b8d84c045f92f1",
            "2b044544f82c42ee8256534637b7a89b",
            "61155fa4a75c4684b884649e3ca80907",
            "7aeae9e4bb604686938985e266f39d66",
            "9539ab44dcdd41d3932e150e40313a20",
            "8d6fed00342a41de8e9e90707fae2a95",
            "d15f964d12714aaea7a6acb35d1bf0be",
            "8de7bd0bce5f4085afedbc29ac821f01",
            "e8b310725d974ce3b69b58f4dfac6ca7",
            "5d408eea61c6466da7d3ff620efbc1f8",
            "00144b41e1d747a5baaf52db4332fa82",
            "67a54708f3af4d009f6cb1d1c01c1a82",
            "20aeb2f20f5c47e883c26ae6ba57798b",
            "7f2aec26bf8647bdbf12b77090c94ef7",
            "f1fb14edd05042f09c4aa71d7a0679a6",
            "50106076d6dd4c65be987fc90c376f42",
            "19843e1499b7424a9b0a5c83cbb02d9e",
            "9db3f3f8294a40bdada3b53442c374d7",
            "aef936926be045ac920fc5e5a5515726",
            "a3ab5f15b9264dcba8408a7e49090745",
            "655e4da8972a4e35b594b1972a7cb8a4",
            "6bb61c4ed27b42f48eaa47c42cf26959",
            "f61ff5e1f9cc47c8a9d90bf447c4ae4b",
            "4ced0ba56cb8464c8858a716e638ecdb",
            "6615725b0e2c42ea89d341513f3ed872",
            "d7f3d8cd924f458a92a4f7cd0db1b09c",
            "ed7c9c83368f4853918d7e8fb5df9767",
            "d47679bc52634b699eed7340ea23d240",
            "30a0133eba494f59b972f4b45ed169e9",
            "99c55926938346b7a51899eac643a5b1",
            "bcc2ea4eff7c42b8a1fb6b30fc7f2cf8",
            "b9aed2b27a0c4232b7f7d12a3d6ed0a8",
            "d4fe34c125774c279d1709dabfb0d280",
            "6db41433ebc342f387faf6796b82733e",
            "a2b6bd0e5e5f487ca2798ce8bae73587",
            "15ab5bb3dbc7445da75019f58b6d86f3",
            "5909f8a4b6ec49678123829dc3dea731",
            "ceb8470a0b3c456ab08e4545b4a7f961",
            "dde29cc5a2ff4c5ab7ee011eccbf4b6d",
            "bfaed0bf96e149e9a1dbafb7a67eb1a6",
            "65714250696047b687554ad37e23d149",
            "9bed98b48e324ae6b1e76cf06a9666e4",
            "aa50fe997a454f58b33c96c4ce1798be",
            "80518aca84094faa842fcb0dc5d27161",
            "064e6e7e1d2e45068f9612a90e16f5e2",
            "ec4d93a1484446b980f2724a5f98e45d",
            "4750ddbc64364ac69319f2a5d4124263",
            "9129eb28e02444a9b6c87899674d46b7",
            "5b841e0b3c284ef085c967c8a64caf0b",
            "677381ed372e46ea8fe3e9c41b6b84a8",
            "d294b897f0c341f8a9807b83314b29c7",
            "0188855190714cfcaee89c3f510a67b7",
            "d5531fccfb4347b899878aa556906bad",
            "b7a844149b63402183457473e99a7512",
            "e6d02c6b74d542f498d9c49701519c66",
            "823023501ca44e9d8d154dc21316cc22",
            "95c9fe01b86a4d8885acde14eb161593",
            "b08ee0aeb9db44b8bac23da7e64279a2"
          ]
        },
        "id": "M-aGSvsDYlzN",
        "outputId": "95a2f9cf-eb44-4861-b2af-ecfe675349bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91627c55a3df485b9bd6850f9ae76265"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6dfd231b76c403c99e50b2ba15f0d50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7d659b4a41d4ce3a32df1b27c07804c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/9.93G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8113d8bbb5834056a78606d332a3049a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/9.81G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a272c287e4f4a1cb01c0e0ed24e38b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7aeae9e4bb604686938985e266f39d66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1fb14edd05042f09c4aa71d7a0679a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7f3d8cd924f458a92a4f7cd0db1b09c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5909f8a4b6ec49678123829dc3dea731"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9129eb28e02444a9b6c87899674d46b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model pythia-12b into HookedTransformer\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 79.32 GiB of which 89.88 MiB is free. Process 3531 has 79.22 GiB memory in use. Of the allocated memory 78.73 GiB is allocated by PyTorch, and 6.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1080023579.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHookedTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pythia-12b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-609777421.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(model, prompt, layer, n_heads, graph)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mstr_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_str_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_with_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;31m# top 5 predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mrun_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHookedRootModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \"\"\"\n\u001b[0;32m--> 702\u001b[0;31m         out, cache_dict = super().run_with_cache(\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mrun_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0mclear_contexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_contexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         ):\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mincl_bwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mmodel_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    618\u001b[0m                     )\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                 residual = block(\n\u001b[0m\u001b[1;32m    621\u001b[0m                     \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                     \u001b[0;31m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformer_lens/components/transformer_block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# queries, keys and values, independently.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.attn(\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0mquery_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformer_lens/components/abstract_attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \"\"\"\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_qkv_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpast_kv_cache_entry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformer_lens/components/abstract_attention.py\u001b[0m in \u001b[0;36mcalculate_qkv_matrices\u001b[0;34m(self, query_input, key_input, value_input)\u001b[0m\n\u001b[1;32m    395\u001b[0m             )\n\u001b[1;32m    396\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_Q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_Q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_in_4bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_K\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParams4bit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformer_lens/utilities/attention.py\u001b[0m in \u001b[0;36msimple_attn_linear\u001b[0;34m(input, w, b)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"head_index d_model d_head -> (head_index d_head) d_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mb_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"head_index d_head -> (head_index d_head)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/einops/einops.py\u001b[0m in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \"\"\"\n\u001b[0;32m--> 600\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rearrange\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/einops/einops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mrecipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_transformation_recipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         return _apply_recipe(\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhashable_axes_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/einops/einops.py\u001b[0m in \u001b[0;36m_apply_recipe\u001b[0;34m(backend, recipe, tensor, reduction_type, axes_lengths)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_axes_w_added\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos2len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madded_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_shapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/einops/_backends.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(self, x, shape)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 79.32 GiB of which 89.88 MiB is free. Process 3531 has 79.22 GiB memory in use. Of the allocated memory 78.73 GiB is allocated by PyTorch, and 6.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    }
  ]
}
